{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENTS_DIR = './documents'\n",
    "VECTOR_DB_DIR = './vector_db'\n",
    "#MODEL_DIR = './models'\n",
    "NUM_RELEVANT_DOCS = 3\n",
    "EMBEDDING_MODEL = 'paraphrase-MiniLM-L3-v2'\n",
    "# LLM = 'distilbert/distilgpt2'\n",
    "LLM = 'meta/llama-3.1-8b-instruct'\n",
    "CLASSIFICATION_MODEL = \"facebook/bart-large-mnli\"\n",
    "\n",
    "query = \"I'm not able to play video smoothly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading API Keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "support_tickets = load_json('support_tickets.json')\n",
    "knowledge_base = load_json('knowledge_base.json')\n",
    "\n",
    "# docs = [Document(page_content=entry[\"content\"]) for entry in knowledge_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for ticket, entry in zip(support_tickets, knowledge_base):\n",
    "    ticket_text = ticket['text']\n",
    "    relevant_knowledge = entry['content']\n",
    "    \n",
    "    combined_content = f\"Support Ticket: {ticket_text}\\nRelevant Knowledge: {relevant_knowledge}\"\n",
    "    \n",
    "    docs.append(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in docs:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=combined_content) for combined_content in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build(docs, vector_db_dir, embedding_model, embedding_model_name):\n",
    "#     os.makedirs(vector_db_dir, exist_ok=True)\n",
    "#     vector_db_path = os.path.join(vector_db_dir, embedding_model_name.split('/')[-1].split('.')[0])\n",
    "\n",
    "#     if not os.path.exists(vector_db_path):\n",
    "#         db = Chroma.from_documents(\n",
    "#             documents=docs, \n",
    "#             embedding=embedding_model, \n",
    "#             persist_directory=vector_db_path\n",
    "#         )\n",
    "#         print(f'Vector Database Created at {vector_db_path}')\n",
    "    \n",
    "#     else:\n",
    "#         db = Chroma(\n",
    "#             persist_directory=vector_db_path,\n",
    "#             embedding_function=embedding_model\n",
    "#         )\n",
    "#         print(f'Vector Database already exists at {vector_db_path}')\n",
    "\n",
    "#     return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/architgpt28/miniconda3/envs/EdLight_RAG/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/architgpt28/miniconda3/envs/EdLight_RAG/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(VECTOR_DB_DIR, exist_ok=True)\n",
    "vector_db_path = os.path.join(VECTOR_DB_DIR, EMBEDDING_MODEL.split('/')[-1])\n",
    "\n",
    "if not os.path.exists(vector_db_path):\n",
    "\n",
    "    print(f'Creating Vector Database at {vector_db_path}')\n",
    "    Chroma.from_documents(\n",
    "        documents=docs, \n",
    "        embedding=embedding_model, \n",
    "        persist_directory=vector_db_path\n",
    "    )\n",
    "\n",
    "db = Chroma(\n",
    "    persist_directory=vector_db_path,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":NUM_RELEVANT_DOCS}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/architgpt28/miniconda3/envs/EdLight_RAG/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Ticket: The video playback is very laggy on my device.\n",
      "Relevant Knowledge: Category 5 - Performance Issues - Performance issues can be related to device specifications, network connectivity, or app optimization.\n",
      "Support Ticket: I can't find the option to change my profile picture.\n",
      "Relevant Knowledge: Category 4 - Account Management - Account management includes tasks such as changing profile information, linking social media accounts, and managing privacy settings.\n",
      "Support Ticket: The app crashes every time I try to upload a photo.\n",
      "Relevant Knowledge: Category 2 - App Functionality - App crashes can be caused by outdated software or device incompatibility.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(QUERY, str):\n",
    "#     relevant_docs = retriever.invoke(QUERY)\n",
    "\n",
    "#     print('\\nRelevant Documents:')\n",
    "#     for i, doc in enumerate(relevant_docs):\n",
    "#         print(f'Document {i}:\\n\\n{doc.page_content}\\n')\n",
    "#         if doc.metadata:\n",
    "#             print(f\"Source: {doc.metadata.get('Header 1', 'None')} -> {doc.metadata.get('Header 2', 'None')} -> {doc.metadata.get('Header 3', 'None')} -> {doc.metadata.get('Header 4', 'None')}\\n\")\n",
    "\n",
    "#     times = []\n",
    "#     for _ in tqdm(range(1000)):\n",
    "#         start = time.time()\n",
    "#         retriever.invoke(QUERY)\n",
    "#         end = time.time()\n",
    "\n",
    "#         times.append(end - start)\n",
    "\n",
    "#     print(f'\\nAverage Retreival Runtime: {(sum(times)/len(times))*1000} MilliSeconds')\n",
    "\n",
    "#     sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=LLM,\n",
    "#     task=\"text-generation\",\n",
    "#         pipeline_kwargs=dict(\n",
    "#         max_new_tokens=10,\n",
    "#         do_sample=False,\n",
    "#         repetition_penalty=1.03,\n",
    "#         temperature=0,\n",
    "#     ),\n",
    "#     model_kwargs=dict(\n",
    "#         cache_dir=\"./cache\"\n",
    "#     ),\n",
    "#     # trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define a prompt template for answering questions with context\n",
    "# qa_system_prompt = (\n",
    "#     \"You are an assistant for question-answering tasks. Based on the following document, provide relevant knowledge to answer the query. If the document doesn't contain relevant information, say that you don't know. In the answer only give the relevant knowledge corresponding to the ticket.\\n\\nDocument:\\n{document}\\n\\nQuery:\\n{query}\"\n",
    "# )\n",
    "\n",
    "# qa_prompt = PromptTemplate(\n",
    "#     input_variables=[\"query\", \"document\"],\n",
    "#     template=qa_system_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You are an assistant for question-answering tasks. Based on the following document, provide relevant knowledge to answer the query. If the document doesn't contain relevant information, say that you don't know. In the answer only give the relevant knowledge corresponding to the ticket.\\n\\nDocument:\\nSupport Ticket: My account login is not working. I've tried resetting my password twice.\\nRelevant Knowledge: Category 1 - Login Issues - Login issues often occur due to incorrect passwords or account lockouts.\\n\\nQuery:\\nI can't log into my account and it's not working. It's not working\", \"You are an assistant for question-answering tasks. Based on the following document, provide relevant knowledge to answer the query. If the document doesn't contain relevant information, say that you don't know. In the answer only give the relevant knowledge corresponding to the ticket.\\n\\nDocument:\\nSupport Ticket: I can't find the option to change my profile picture.\\nRelevant Knowledge: Category 4 - Account Management - Account management includes tasks such as changing profile information, linking social media accounts, and managing privacy settings.\\n\\nQuery:\\nI can't log into my account.\\nRelevant Knowledge: Category 5 - Account\"]\n"
     ]
    }
   ],
   "source": [
    "# def retrieval_qa_chain(query):\n",
    "#     # Retrieve relevant documents\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "#     # Initialize the results list\n",
    "#     answers = []\n",
    "\n",
    "#     # Process each document with the QA chain\n",
    "#     for doc in docs:\n",
    "#         # Create a formatted prompt with the document and query\n",
    "#         prompt = qa_prompt.format(query=query, document=doc.page_content)\n",
    "#         # Get the answer from the LLM\n",
    "#         answer = llm(prompt)\n",
    "#         answers.append(answer)\n",
    "    \n",
    "#     return answers\n",
    "\n",
    "# # Example query\n",
    "# query = \"I can't log into my account\"\n",
    "# answers = retrieval_qa_chain(query)\n",
    "# # print(answers)\n",
    "# for answer in answers:\n",
    "#     print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_system_prompt = \"\"\"You are an assistant for classifying support tickets. Based on the following retrieved support tickets and their corresponding knowledge base categories, classify the given query into the most similar category. Only return the category name and number, nothing else.\n",
    "\n",
    "# Retrieved tickets and categories:\n",
    "# {retrieved_docs}\n",
    "\n",
    "# Query to classify: {query}\n",
    "\n",
    "# Classify the query and respond with only the category name and number. For example, if the query is similar to a login issue, respond with just 'Category 1 - Login Issues'. Only respond from what is mentioned in relevant knowledge and do not generate new text\"\"\"\n",
    "\n",
    "# classification_prompt = PromptTemplate(\n",
    "#     input_variables=[\"query\", \"retrieved_docs\"],\n",
    "#     template=classification_system_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_ticket(query, retrieved_docs):\n",
    "#     # Format the retrieved documents\n",
    "#     formatted_docs = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "#     # Create a formatted prompt with the query and retrieved documents\n",
    "#     prompt = classification_prompt.format(query=query, retrieved_docs=formatted_docs)\n",
    "    \n",
    "#     # Get the classification from the LLM\n",
    "#     classification = llm(prompt)\n",
    "    \n",
    "#     return classification.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification = classify_ticket(query, retrieved_docs)\n",
    "# print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "llm = ChatNVIDIA(model = LLM, temperature=0, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_system_prompt = \"\"\"You are an assistant for classifying support tickets. Based on the following retrieved support tickets and their corresponding knowledge base categories, classify the given query into the most similar category. Only return the category name and number, nothing else.\n",
    "\n",
    "Retrieved tickets and categories:\n",
    "{retrieved_docs}\n",
    "\n",
    "Query to classify: {query}\n",
    "\n",
    "Classify the query and respond with only the category name and number. For example, if the query is similar to a login issue, respond with just 'Category 1 - Login Issues'. Only respond from what is mentioned in relevant knowledge and do not generate new text\"\"\"\n",
    "\n",
    "classification_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"retrieved_docs\"],\n",
    "    template=classification_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ticket(query, retrieved_docs):\n",
    "    # Format the retrieved documents\n",
    "    formatted_docs = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    # Create a formatted prompt with the query and retrieved documents\n",
    "    prompt = classification_prompt.format(query=query, retrieved_docs=formatted_docs)\n",
    "    \n",
    "    # Get the classification from the LLM\n",
    "    classification = llm(prompt)\n",
    "    \n",
    "    return classification.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the retrieved tickets and categories, I would classify the query as:\\n\\nCategory 5 - Video Playback Issues' response_metadata={'role': 'assistant', 'content': 'Based on the retrieved tickets and categories, I would classify the query as:\\n\\nCategory 5 - Video Playback Issues', 'token_usage': {'prompt_tokens': 141, 'total_tokens': 163, 'completion_tokens': 22}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-1cb5d172-c103-4fd1-98b9-19d3bd6cbecb-0' role='assistant'\n",
      "\n",
      "Model Runtime: 0.433307409286499 MilliSeconds\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "prompt=query\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=classification_system_prompt),\n",
    "    HumanMessage(content=prompt)\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "classification = llm(messages)\n",
    "end = time.time()\n",
    "\n",
    "print(classification)\n",
    "\n",
    "time = end - start\n",
    "print(f'\\nModel Runtime: {time} Seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the retrieved tickets and categories, I would classify the query as:\\n\\nCategory 5 - Video Playback Issues' response_metadata={'role': 'assistant', 'content': 'Based on the retrieved tickets and categories, I would classify the query as:\\n\\nCategory 5 - Video Playback Issues', 'token_usage': {'prompt_tokens': 141, 'total_tokens': 163, 'completion_tokens': 22}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run-b6022f5f-b955-4447-8fc9-d5baa498dd0d-0' role='assistant'\n"
     ]
    }
   ],
   "source": [
    "#classification = classify_ticket(query, retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EdLight_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
